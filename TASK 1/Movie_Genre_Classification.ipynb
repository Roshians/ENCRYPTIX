{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q_yc_DgYUZe",
        "outputId": "16076101-d8ef-4862-a3cb-e48f035a3fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer  # Optional: for stemming\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "# Load Training Data\n",
        "train_path = \"/content/gdrive/MyDrive/Genre Classification Dataset/train_data.txt\"\n",
        "train_data = pd.read_csv(train_path, sep=':::', names=['Title', 'Genre', 'Description'], engine='python')\n",
        "\n",
        "# Load Test Data\n",
        "test_path = \"/content/gdrive/MyDrive/Genre Classification Dataset/test_data.txt\"\n",
        "test_data = pd.read_csv(test_path, sep=':::', names=['Id', 'Title', 'Description'], engine='python')\n",
        "\n",
        "stemmer = LancasterStemmer()  # Optional: for stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "  This function performs all preprocessing steps on text data.\n",
        "  \"\"\"\n",
        "  # text = text.lower()  # Lowercase all characters\n",
        "  text = re.sub(r'@\\S+', '', text)  # Remove Twitter handles\n",
        "  text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "  text = re.sub(r'pic.\\S+', '', text)\n",
        "  text = re.sub(r\"[^a-zA-Z+']\", ' ', text)  # Keep only characters\n",
        "  text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text + ' ')  # Keep words with length > 1 only\n",
        "  text = \"\".join([i for i in text if i not in string.punctuation])\n",
        "  words = nltk.word_tokenize(text)\n",
        "  words = [word for word in words if word not in stop_words and len(word) > 2]  # Remove stopwords and short words\n",
        "  # Optional: Apply stemming if desired\n",
        "  # words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "  return \" \".join(words).strip()  # Remove spaces and return preprocessed text\n",
        "\n",
        "\n",
        "train_data['Tokens'] = train_data['Description'].apply(preprocess_text)\n",
        "test_data['Tokens'] = test_data['Description'].apply(preprocess_text)\n",
        "# print(train_data.head())\n"
      ],
      "metadata": {
        "id": "l86Oy_OIeIER"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "\n",
        "tf_vector = TfidfVectorizer()\n",
        "\n",
        "X_train = tf_vector.fit_transform(train_data[\"Tokens\"])\n",
        "X_test = tf_vector.transform(test_data[\"Tokens\"])\n",
        "dic_vocabulary = tf_vector.vocabulary_\n",
        "# X_names = tf_vector.get_feature_names_out()\n",
        "\n",
        "X = X_train\n",
        "y = train_data[\"Genre\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "lr_y_pred = model_lr.predict(X_val)\n",
        "lr_accuracy = accuracy_score(y_val, lr_y_pred)\n",
        "print(\"Logistic Regression Accuracy on Validation Set:\", lr_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXFyPAI6zom3",
        "outputId": "b1a74402-c67e-4582-fed5-a47fe476ad09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy on Validation Set: 0.5823111684958038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}